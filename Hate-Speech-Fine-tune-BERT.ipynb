{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"Hate Speech/labeled_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.read_csv(DATA_PATH)\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=pandas_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>!!! RT : As a woman you shouldn't complain abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>!!!!! RT : boy dats cold...tyga dwn bad for cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>!!!!!!! RT Dawg!!!! RT : You ever fuck a bitch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>!!!!!!!!! RT _G_Anderson: _based she look like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>!!!!!!!!!!!!! RT : The shit you hear about me ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                       tweet_cleaned  \n",
       "0  !!! RT : As a woman you shouldn't complain abo...  \n",
       "1  !!!!! RT : boy dats cold...tyga dwn bad for cu...  \n",
       "2  !!!!!!! RT Dawg!!!! RT : You ever fuck a bitch...  \n",
       "3  !!!!!!!!! RT _G_Anderson: _based she look like...  \n",
       "4  !!!!!!!!!!!!! RT : The shit you hear about me ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df['tweet_cleaned'] = pandas_df['tweet'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither', 'class', 'tweet', 'tweet_cleaned'],\n",
       "    num_rows: 24783\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(pandas_df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned'],\n",
       "        num_rows: 18587\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned'],\n",
       "        num_rows: 1549\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned'],\n",
       "        num_rows: 4647\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_valid = ds.train_test_split()\n",
    "\n",
    "test_valid = train_test_valid['test'].train_test_split()\n",
    "\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']\n",
    "    })\n",
    "\n",
    "\n",
    "dataset = train_test_valid_dataset.remove_columns(['hate_speech', 'offensive_language', 'neither','Unnamed: 0', 'count'])\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2066, 9444, 22559, 2734, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Just checking tokenization\"\n",
    "\n",
    "output = tokenizer(text)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Just', 'checking', 'token', '##ization', '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(output['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: [CLS] Just checking tokenization [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenized text: {tokenizer.convert_tokens_to_string(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is : 28996\n",
      "Model max length is : 512\n",
      "Model input names are: ['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size is : {tokenizer.vocab_size}\")\n",
    "\n",
    "print(f\"Model max length is : {tokenizer.model_max_length}\")\n",
    "\n",
    "print(f\"Model input names are: {tokenizer.model_input_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned'],\n",
       "        num_rows: 18587\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned'],\n",
       "        num_rows: 1549\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned'],\n",
       "        num_rows: 4647\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f188777a2734a9396fe8cede09c2e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18587 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b8739591be463cb71b14bbcb21445f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d843c4d97640fcb2c2eae617179117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 18587\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1549\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['class', 'tweet', 'tweet_cleaned', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4647\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(train_dataset):\n",
    "    return tokenizer(train_dataset['tweet_cleaned'], padding='max_length', truncation=True) \n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset\n",
    "# train_dataset = tokenized_dataset['train']\n",
    "# eval_dataset = tokenized_dataset['valid']\n",
    "# test_dataset = tokenized_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': Value(dtype='int64', id=None),\n",
       " 'tweet': Value(dtype='string', id=None),\n",
       " 'tweet_cleaned': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].remove_columns(['tweet', \"tweet_cleaned\"])\n",
    "tokenized_dataset[\"valid\"] = tokenized_dataset[\"valid\"].remove_columns(['tweet', \"tweet_cleaned\"])\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].remove_columns(['tweet', \"tweet_cleaned\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].rename_column(\"class\",\"label\")\n",
    "tokenized_dataset[\"valid\"] = tokenized_dataset[\"valid\"].rename_column(\"class\",\"label\")\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].rename_column(\"class\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].with_format(\"torch\")\n",
    "# tokenized_dataset[\"valid\"] = tokenized_dataset[\"valid\"].with_format(\"torch\")\n",
    "# tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset[\"train\"][\"label\"].isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = train_dataset.remove_columns(['tweet', \"tweet_cleaned\"])\n",
    "\n",
    "# tf_eval_dataset = eval_dataset.remove_columns(['tweet', \"tweet_cleaned\"])\n",
    "\n",
    "# tf_test_dataset = test_dataset.remove_columns(['tweet', \"tweet_cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased',num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = { x: train_set[x] for x in tokenizer.model_input_names }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = data_collator(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"BERT_FINE_TUNE\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_399027/2699458560.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = dataset[\"train\"].features['class']\n",
    "# label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "def compute_metric(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels,preds,average='weighted')\n",
    "    acc = accuracy_score(labels,preds)\n",
    "    return {'accuracy':acc,'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    "    # data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3486' max='3486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3486/3486 42:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.281973</td>\n",
       "      <td>0.905961</td>\n",
       "      <td>0.906372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.393018</td>\n",
       "      <td>0.908758</td>\n",
       "      <td>0.903645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.451770</td>\n",
       "      <td>0.909404</td>\n",
       "      <td>0.907293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3486, training_loss=0.1386810002898732, metrics={'train_runtime': 2524.024, 'train_samples_per_second': 22.092, 'train_steps_per_second': 1.381, 'total_flos': 1.4671467285931008e+16, 'train_loss': 0.1386810002898732, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(tokenized_dataset[\"test\"])\n",
    "# print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.07808976,  4.6152897 , -4.242687  ],\n",
       "       [-1.9399294 ,  6.0640926 , -4.1525836 ],\n",
       "       [ 0.05714353,  4.438873  , -4.6816883 ],\n",
       "       ...,\n",
       "       [-2.2098868 ,  6.2985063 , -4.0365047 ],\n",
       "       [-1.7729027 ,  2.594094  , -0.26247784],\n",
       "       [-2.1838024 , -1.9323545 ,  4.8640385 ]], dtype=float32), label_ids=array([1, 1, 1, ..., 1, 2, 1]), metrics={'test_loss': 0.4457743763923645, 'test_accuracy': 0.9141381536475145, 'test_f1': 0.9129102858334662, 'test_runtime': 19.0149, 'test_samples_per_second': 81.463, 'test_steps_per_second': 5.101})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/raj/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:254\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[1;32m    255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m predict_score_and_class_dict \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mHate Speech\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m  \u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mOffensive Language\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m  \u001b[39m2\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mNeither\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m preds \u001b[39m=\u001b[39m model(tokenizer([\u001b[39m\"\u001b[39;49m\u001b[39mHe is useless, I dont know why he came to our neighbourhood\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mThat guy sucks\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mHe is such a retard\u001b[39;49m\u001b[39m\"\u001b[39;49m],return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(preds)\n\u001b[1;32m      9\u001b[0m class_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(preds, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/raj/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1563\u001b[0m     input_ids,\n\u001b[1;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1572\u001b[0m )\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/raj/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:968\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    969\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/raj/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:256\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[1;32m    255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# predict_score_and_class_dict = {0: 'Hate Speech',\n",
    "#  1: 'Offensive Language',\n",
    "#  2: 'Neither'}\n",
    "\n",
    "# preds = model(tokenizer([\"He is useless, I dont know why he came to our neighbourhood\", \"That guy sucks\", \"He is such a retard\"],return_tensors=\"pt\",padding=True,truncation=True))['logits']\n",
    "\n",
    "# print(preds)\n",
    "\n",
    "# class_preds = np.argmax(preds, axis=1)\n",
    "\n",
    "# for pred in class_preds:\n",
    "#   print(predict_score_and_class_dict[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
